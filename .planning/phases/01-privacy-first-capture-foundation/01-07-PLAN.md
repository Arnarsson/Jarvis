---
phase: 01-privacy-first-capture-foundation
plan: 07
type: execute
wave: 3
depends_on: ["01-03"]
files_modified:
  - agent/src/jarvis/sync/__init__.py
  - agent/src/jarvis/sync/uploader.py
  - agent/src/jarvis/sync/queue.py
autonomous: true

must_haves:
  truths:
    - "Captures can be uploaded to server with retry on failure"
    - "Failed uploads are queued locally for later retry"
    - "Queue persists across agent restarts"
  artifacts:
    - path: "agent/src/jarvis/sync/uploader.py"
      provides: "Async HTTP uploader"
      contains: "class CaptureUploader"
    - path: "agent/src/jarvis/sync/queue.py"
      provides: "Persistent local queue for offline captures"
      contains: "class UploadQueue"
  key_links:
    - from: "agent/src/jarvis/sync/uploader.py"
      to: "httpx"
      via: "Async HTTP client"
      pattern: "httpx\\.AsyncClient"
    - from: "agent/src/jarvis/sync/queue.py"
      to: "sqlite"
      via: "Persistent queue storage"
      pattern: "sqlite3"
---

<objective>
Implement agent-to-server upload with retry logic and offline queue.

Purpose: Ensures captures reliably reach the server even when network is unstable, supporting the upload requirement of CAPT-06.
Output: Async uploader with exponential backoff retry and SQLite-backed persistent queue.
</objective>

<execution_context>
@/home/sven/.claude/get-shit-done/workflows/execute-plan.md
@/home/sven/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-privacy-first-capture-foundation/01-RESEARCH.md
@.planning/phases/01-privacy-first-capture-foundation/01-03-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Async HTTP uploader with retry</name>
  <files>
    agent/src/jarvis/sync/__init__.py
    agent/src/jarvis/sync/uploader.py
  </files>
  <action>
Implement capture uploader using httpx:

1. Create `agent/src/jarvis/sync/uploader.py`:
   - Dataclass `UploadResult`:
     - success: bool
     - capture_id: str | None
     - error: str | None
     - attempts: int

   - Class `CaptureUploader`:
     - `__init__(self, server_url: str, max_retries: int = 3, timeout: float = 30.0)`:
       - Stores configuration
       - Creates reusable httpx.AsyncClient (connection pooling)

     - `async def upload(self, filepath: Path, metadata: dict) -> UploadResult`:
       - Attempts upload with exponential backoff retry
       - For each attempt:
         1. Open file, create multipart form
         2. POST to {server_url}/api/captures
         3. On success: return UploadResult(success=True, capture_id=response.id)
         4. On 5xx or connection error: sleep(2 ** attempt), retry
         5. On 4xx: return failure immediately (no retry)
       - After max_retries: return UploadResult(success=False, error=...)

     - `async def upload_bytes(self, data: bytes, filename: str, metadata: dict) -> UploadResult`:
       - Same as upload but takes bytes directly (for in-memory captures)

     - `async def check_server(self) -> bool`:
       - GET {server_url}/health/ready
       - Returns True if 200, False otherwise

     - `async def close(self)`:
       - Closes the httpx client

     - `async def __aenter__` and `__aexit__` for async context manager

2. Create `agent/src/jarvis/sync/__init__.py` exporting CaptureUploader

Use httpx.AsyncClient for connection reuse. Set reasonable timeouts.
Include User-Agent header identifying jarvis-agent version.
  </action>
  <verify>
cd /home/sven/Documents/jarvis/agent && python -c "
import asyncio
from jarvis.sync.uploader import CaptureUploader, UploadResult

async def test():
    uploader = CaptureUploader('http://localhost:8000')
    # Server not running, so this should fail gracefully
    available = await uploader.check_server()
    print(f'Server available: {available}')
    await uploader.close()

asyncio.run(test())
"
  </verify>
  <done>CaptureUploader can attempt uploads and handles connection errors gracefully</done>
</task>

<task type="auto">
  <name>Task 2: Persistent upload queue</name>
  <files>
    agent/src/jarvis/sync/queue.py
  </files>
  <action>
Implement SQLite-backed queue for offline captures:

1. Create `agent/src/jarvis/sync/queue.py`:
   - Dataclass `QueuedCapture`:
     - id: str (UUID)
     - filepath: str
     - metadata_json: str
     - created_at: datetime
     - attempts: int
     - last_attempt: datetime | None
     - status: str ("pending", "uploading", "failed")

   - Class `UploadQueue`:
     - `__init__(self, db_path: Path)`:
       - Opens SQLite database
       - Creates queue table if not exists:
         - id TEXT PRIMARY KEY
         - filepath TEXT
         - metadata_json TEXT
         - created_at TEXT
         - attempts INTEGER DEFAULT 0
         - last_attempt TEXT
         - status TEXT DEFAULT 'pending'

     - `def enqueue(self, filepath: Path, metadata: dict) -> str`:
       - Generates UUID
       - Inserts record with status='pending'
       - Returns queue item ID

     - `def get_pending(self, limit: int = 10) -> list[QueuedCapture]`:
       - Returns pending items ordered by created_at
       - Skips items attempted in last 60 seconds (backoff)

     - `def mark_uploading(self, item_id: str)`:
       - Sets status='uploading', increments attempts

     - `def mark_completed(self, item_id: str)`:
       - Deletes item from queue

     - `def mark_failed(self, item_id: str, error: str)`:
       - Sets status='failed' if attempts >= max_attempts
       - Otherwise sets status='pending' for retry

     - `def get_stats(self) -> dict`:
       - Returns counts by status

     - `def cleanup_old(self, days: int = 7)`:
       - Removes failed items older than N days

     - `def close(self)`:
       - Closes database connection

Use sqlite3 from standard library (synchronous is fine for local queue).
Queue file stored in agent data directory (~/.local/share/jarvis/queue.db).
  </action>
  <verify>
cd /home/sven/Documents/jarvis/agent && python -c "
from pathlib import Path
from jarvis.sync.queue import UploadQueue
import tempfile

with tempfile.TemporaryDirectory() as tmpdir:
    queue = UploadQueue(Path(tmpdir) / 'test.db')

    # Enqueue item
    item_id = queue.enqueue(Path('/tmp/test.jpg'), {'timestamp': '2024-01-01'})
    print(f'Enqueued: {item_id}')

    # Get pending
    pending = queue.get_pending()
    print(f'Pending: {len(pending)} items')

    # Mark uploading then completed
    queue.mark_uploading(item_id)
    queue.mark_completed(item_id)

    pending = queue.get_pending()
    print(f'After complete: {len(pending)} items')

    queue.close()
"
  </verify>
  <done>UploadQueue persists items, supports status transitions, survives restart</done>
</task>

</tasks>

<verification>
After both tasks complete:
1. CaptureUploader retries failed uploads with exponential backoff
2. UploadQueue persists across process restarts
3. Failed uploads are queued for later retry
4. Queue tracks attempt counts for backoff logic
</verification>

<success_criteria>
- Uploads retry on transient failures (covers reliability aspect of CAPT-06)
- Offline captures are queued for later sync
- Queue persists in SQLite for durability
- No data loss when network is unavailable
</success_criteria>

<output>
After completion, create `.planning/phases/01-privacy-first-capture-foundation/01-07-SUMMARY.md`
</output>
