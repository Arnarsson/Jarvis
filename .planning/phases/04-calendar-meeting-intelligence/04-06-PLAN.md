---
phase: 04-calendar-meeting-intelligence
plan: 06
type: execute
wave: 3
depends_on: [04-05]
files_modified:
  - server/pyproject.toml
  - server/src/jarvis_server/transcription/__init__.py
  - server/src/jarvis_server/transcription/whisper.py
  - server/src/jarvis_server/transcription/tasks.py
  - server/src/jarvis_server/processing/tasks.py
autonomous: true

must_haves:
  truths:
    - "Audio files are transcribed using faster-whisper"
    - "Transcription includes timestamps for each segment"
    - "Transcription runs as background ARQ task"
    - "Transcription result stored in Meeting.transcript"
  artifacts:
    - path: "server/src/jarvis_server/transcription/whisper.py"
      provides: "faster-whisper transcription wrapper"
      contains: "WhisperModel"
    - path: "server/src/jarvis_server/transcription/tasks.py"
      provides: "ARQ transcription task"
      contains: "transcribe_meeting_task"
  key_links:
    - from: "server/src/jarvis_server/transcription/whisper.py"
      to: "faster_whisper"
      via: "WhisperModel import"
      pattern: "from faster_whisper import WhisperModel"
    - from: "server/src/jarvis_server/transcription/tasks.py"
      to: "server/src/jarvis_server/transcription/whisper.py"
      via: "TranscriptionService import"
      pattern: "from.*whisper.*import"
---

<objective>
Implement speech-to-text transcription using faster-whisper

Purpose: Transcribe meeting audio files to text using faster-whisper, a GPU-accelerated Whisper implementation. Transcription runs as a background task to avoid blocking. This fulfills CAL-05 requirement.

Output: Transcription service and ARQ task for processing meeting audio
</objective>

<execution_context>
@/home/sven/.claude/get-shit-done/workflows/execute-plan.md
@/home/sven/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/04-calendar-meeting-intelligence/04-RESEARCH.md
@.planning/phases/04-calendar-meeting-intelligence/04-05-SUMMARY.md
@server/src/jarvis_server/processing/tasks.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add faster-whisper dependency</name>
  <files>server/pyproject.toml</files>
  <action>
Add faster-whisper to server dependencies.

Update server/pyproject.toml:
"faster-whisper>=1.0.0",

Note: faster-whisper requires:
- CUDA 12 + cuDNN 9 for GPU acceleration
- Falls back to CPU if GPU not available (slower but works)

The package uses CTranslate2 backend for optimized inference.
  </action>
  <verify>
cd /home/sven/Documents/jarvis && pip install -e server/ && python -c "from faster_whisper import WhisperModel; print('faster-whisper installed')"
  </verify>
  <done>faster-whisper dependency installed</done>
</task>

<task type="auto">
  <name>Task 2: Create transcription service module</name>
  <files>
    server/src/jarvis_server/transcription/__init__.py
    server/src/jarvis_server/transcription/whisper.py
  </files>
  <action>
Create the transcription service using faster-whisper.

server/src/jarvis_server/transcription/__init__.py:
"""Speech-to-text transcription using faster-whisper."""
from jarvis_server.transcription.whisper import TranscriptionService

__all__ = ["TranscriptionService"]

server/src/jarvis_server/transcription/whisper.py:

import os
from pathlib import Path
from typing import Optional
from dataclasses import dataclass
import structlog

logger = structlog.get_logger()

@dataclass
class TranscriptSegment:
    """A segment of transcribed text with timing."""
    start: float
    end: float
    text: str

@dataclass
class TranscriptionResult:
    """Complete transcription result."""
    language: str
    language_probability: float
    duration: float
    segments: list[TranscriptSegment]
    full_text: str

class TranscriptionService:
    """
    Transcription service using faster-whisper.

    Supports GPU acceleration with CUDA, falls back to CPU.
    """

    def __init__(
        self,
        model_size: str = "base",  # Start small, upgrade as needed
        device: Optional[str] = None,
        compute_type: Optional[str] = None
    ):
        """
        Initialize transcription service.

        Args:
            model_size: Whisper model size (tiny, base, small, medium, large-v3)
            device: "cuda" for GPU, "cpu" for CPU, None for auto-detect
            compute_type: "float16" for GPU, "int8" for CPU, None for auto
        """
        from faster_whisper import WhisperModel

        # Auto-detect device if not specified
        if device is None:
            try:
                import torch
                device = "cuda" if torch.cuda.is_available() else "cpu"
            except ImportError:
                device = "cpu"

        # Set compute type based on device
        if compute_type is None:
            compute_type = "float16" if device == "cuda" else "int8"

        self.device = device
        self.model_size = model_size
        self.compute_type = compute_type

        logger.info(
            "transcription_service_init",
            model_size=model_size,
            device=device,
            compute_type=compute_type
        )

        self.model = WhisperModel(
            model_size,
            device=device,
            compute_type=compute_type
        )

    def transcribe(
        self,
        audio_path: Path,
        language: Optional[str] = None,
        beam_size: int = 5,
        vad_filter: bool = True
    ) -> TranscriptionResult:
        """
        Transcribe an audio file.

        Args:
            audio_path: Path to audio file (WAV, MP3, etc.)
            language: Language code (e.g., "en"), None for auto-detect
            beam_size: Beam search size (higher = more accurate, slower)
            vad_filter: Use voice activity detection to skip silence

        Returns:
            TranscriptionResult with segments and full text
        """
        logger.info(
            "transcription_started",
            audio_path=str(audio_path),
            language=language
        )

        segments_iter, info = self.model.transcribe(
            str(audio_path),
            language=language,
            beam_size=beam_size,
            vad_filter=vad_filter,
            vad_parameters=dict(
                threshold=0.5,
                min_speech_duration_ms=250,
                min_silence_duration_ms=500,
                speech_pad_ms=400
            )
        )

        # Collect segments
        segments = []
        full_text_parts = []

        for segment in segments_iter:
            segments.append(TranscriptSegment(
                start=segment.start,
                end=segment.end,
                text=segment.text.strip()
            ))
            full_text_parts.append(segment.text.strip())

        full_text = " ".join(full_text_parts)

        result = TranscriptionResult(
            language=info.language,
            language_probability=info.language_probability,
            duration=info.duration,
            segments=segments,
            full_text=full_text
        )

        logger.info(
            "transcription_completed",
            audio_path=str(audio_path),
            language=info.language,
            duration=info.duration,
            segment_count=len(segments),
            text_length=len(full_text)
        )

        return result

# Singleton instance for reuse
_service: Optional[TranscriptionService] = None

def get_transcription_service() -> TranscriptionService:
    """Get or create transcription service singleton."""
    global _service
    if _service is None:
        model_size = os.getenv("WHISPER_MODEL_SIZE", "base")
        _service = TranscriptionService(model_size=model_size)
    return _service
  </action>
  <verify>
python -c "
from jarvis_server.transcription.whisper import TranscriptionService, TranscriptSegment, TranscriptionResult
print('TranscriptionService imported successfully')

# Can't test actual transcription without audio file, but verify class exists
import inspect
methods = [m for m in dir(TranscriptionService) if not m.startswith('_')]
print(f'TranscriptionService methods: {methods}')
assert 'transcribe' in methods, 'transcribe method missing'
print('TranscriptionService verified')
"
  </verify>
  <done>Transcription service created with faster-whisper</done>
</task>

<task type="auto">
  <name>Task 3: Create transcription ARQ task</name>
  <files>
    server/src/jarvis_server/transcription/tasks.py
    server/src/jarvis_server/processing/tasks.py
  </files>
  <action>
Create the ARQ task for transcribing meeting audio.

server/src/jarvis_server/transcription/tasks.py:

import json
from pathlib import Path
from datetime import datetime, timezone
from sqlalchemy import select
from sqlalchemy.ext.asyncio import AsyncSession
import structlog

from jarvis_server.db.session import async_session_factory
from jarvis_server.calendar.models import Meeting
from jarvis_server.transcription.whisper import get_transcription_service

logger = structlog.get_logger()

async def transcribe_meeting_task(ctx: dict, meeting_id: str) -> dict:
    """
    ARQ task to transcribe meeting audio.

    Args:
        ctx: ARQ context
        meeting_id: ID of meeting to transcribe

    Returns:
        Dict with transcription status and stats
    """
    logger.info("transcription_task_started", meeting_id=meeting_id)

    async with async_session_factory() as db:
        # Get meeting record
        result = await db.execute(
            select(Meeting).where(Meeting.id == meeting_id)
        )
        meeting = result.scalar_one_or_none()

        if not meeting:
            logger.error("transcription_meeting_not_found", meeting_id=meeting_id)
            return {"status": "error", "reason": "meeting_not_found"}

        if not meeting.audio_path:
            logger.error("transcription_no_audio", meeting_id=meeting_id)
            return {"status": "error", "reason": "no_audio_file"}

        audio_path = Path(meeting.audio_path)
        if not audio_path.exists():
            logger.error("transcription_audio_missing", meeting_id=meeting_id, path=str(audio_path))
            meeting.transcript_status = "failed"
            await db.commit()
            return {"status": "error", "reason": "audio_file_missing"}

        # Update status to processing
        meeting.transcript_status = "processing"
        await db.commit()

        try:
            # Get transcription service and transcribe
            service = get_transcription_service()
            result = service.transcribe(audio_path)

            # Store transcript
            meeting.transcript = result.full_text
            meeting.transcript_status = "completed"

            # Store segment data as JSON in a metadata field if we add one
            # For now, just store the full text

            await db.commit()

            logger.info(
                "transcription_task_completed",
                meeting_id=meeting_id,
                language=result.language,
                duration=result.duration,
                text_length=len(result.full_text)
            )

            return {
                "status": "completed",
                "meeting_id": meeting_id,
                "language": result.language,
                "duration": result.duration,
                "text_length": len(result.full_text),
                "segment_count": len(result.segments)
            }

        except Exception as e:
            logger.exception("transcription_task_failed", meeting_id=meeting_id)
            meeting.transcript_status = "failed"
            await db.commit()
            return {"status": "error", "reason": str(e)}

Update server/src/jarvis_server/processing/tasks.py:

Add to imports:
from jarvis_server.transcription.tasks import transcribe_meeting_task

Add to WorkerSettings.functions list:
    functions = [
        process_capture,
        process_conversation,
        sync_calendar_task,
        transcribe_meeting_task,  # ADD THIS
    ]
  </action>
  <verify>
python -c "
from jarvis_server.transcription.tasks import transcribe_meeting_task
from jarvis_server.processing.tasks import WorkerSettings

print('transcribe_meeting_task imported successfully')

# Verify it's in worker functions
func_names = [f.__name__ for f in WorkerSettings.functions]
print(f'Worker functions: {func_names}')
assert 'transcribe_meeting_task' in func_names, 'Task not registered in worker'
print('Transcription task verified in worker')
"
  </verify>
  <done>Transcription ARQ task created and registered</done>
</task>

</tasks>

<verification>
1. pip install -e server/ installs faster-whisper
2. python -c "from jarvis_server.transcription.whisper import TranscriptionService" works
3. TranscriptionService.transcribe() returns TranscriptionResult
4. ARQ task transcribe_meeting_task registered in WorkerSettings
5. Task updates Meeting.transcript and Meeting.transcript_status
</verification>

<success_criteria>
- faster-whisper installed and importable
- TranscriptionService supports GPU (CUDA) and CPU fallback
- VAD filter enabled to skip silence
- Transcription result includes segments with timestamps
- ARQ task handles meeting lookup and status updates
- Failed transcriptions update transcript_status to "failed"
- Model size configurable via WHISPER_MODEL_SIZE env var
</success_criteria>

<output>
After completion, create `.planning/phases/04-calendar-meeting-intelligence/04-06-SUMMARY.md`
</output>
