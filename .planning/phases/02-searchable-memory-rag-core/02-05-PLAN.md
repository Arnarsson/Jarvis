---
phase: 02-searchable-memory-rag-core
plan: 05
type: execute
wave: 2
depends_on: ["02-03"]
files_modified:
  - server/src/jarvis_server/api/captures.py
autonomous: true

must_haves:
  truths:
    - "Uploaded captures are automatically queued for background processing"
    - "Processing does not block the upload response"
    - "Capture is immediately available even before processing completes"
  artifacts:
    - path: "server/src/jarvis_server/api/captures.py"
      provides: "Updated upload endpoint with job enqueue"
      contains: "enqueue_job"
  key_links:
    - from: "server/src/jarvis_server/api/captures.py"
      to: "arq"
      via: "Redis job enqueue"
      pattern: "enqueue_job.*process_capture"
---

<objective>
Integrate upload endpoint with ARQ processing pipeline to automatically queue captures for OCR and embedding.

Purpose: Complete the upload-to-search pipeline. New captures are stored immediately (fast response) and queued for background processing (no blocking).

Output: Updated capture upload endpoint that enqueues ARQ job after successful storage.
</objective>

<execution_context>
@/home/sven/.claude/get-shit-done/workflows/execute-plan.md
@/home/sven/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-searchable-memory-rag-core/02-RESEARCH.md

# ARQ tasks from 02-03
@.planning/phases/02-searchable-memory-rag-core/02-03-PLAN.md

# Existing capture endpoint
@server/src/jarvis_server/api/captures.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Redis connection to app lifespan</name>
  <files>server/src/jarvis_server/main.py</files>
  <action>
Add ARQ Redis pool to app lifespan for job enqueueing.

Update the lifespan context manager in main.py to initialize Redis connection:

```python
from arq import create_pool
from arq.connections import RedisSettings

@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan: startup and shutdown logic."""
    settings = get_settings()
    logger.info("Starting Jarvis server...")

    # Initialize ARQ Redis pool for background job enqueueing
    redis_settings = RedisSettings(
        host=settings.redis_host,
        port=settings.redis_port,
    )
    app.state.arq_pool = await create_pool(redis_settings)
    logger.info(f"ARQ pool connected to {settings.redis_host}:{settings.redis_port}")

    yield

    # Cleanup
    await app.state.arq_pool.close()
    logger.info("Jarvis server shutdown complete")
```

This makes the Redis pool available via `request.app.state.arq_pool` in endpoints.
  </action>
  <verify>
Run `python -c "from jarvis_server.main import create_app; print('Main OK')"` in server/.venv.
Check main.py has arq_pool in lifespan.
  </verify>
  <done>ARQ Redis pool initialized in app lifespan, available as app.state.arq_pool for enqueueing jobs.</done>
</task>

<task type="auto">
  <name>Task 2: Update capture upload to enqueue processing job</name>
  <files>server/src/jarvis_server/api/captures.py</files>
  <action>
Modify the capture upload endpoint to enqueue ARQ job after storing.

Update the upload endpoint in captures.py:

```python
from fastapi import APIRouter, Depends, File, Form, HTTPException, Request, UploadFile
# ... existing imports ...

@router.post("/", response_model=CaptureResponse, status_code=201)
async def upload_capture(
    request: Request,  # Add request parameter to access app state
    file: UploadFile = File(...),
    timestamp: str = Form(...),
    monitor_index: int = Form(default=0),
    width: int = Form(...),
    height: int = Form(...),
    db: AsyncSession = Depends(get_db),
    storage: FileStorage = Depends(get_storage),
) -> CaptureResponse:
    """Upload a captured screenshot.

    The capture is stored immediately and queued for background processing
    (OCR and embedding). The response returns immediately without waiting
    for processing to complete.
    """
    # ... existing validation and storage logic ...

    # After successful database commit, queue for background processing
    try:
        arq_pool = request.app.state.arq_pool
        await arq_pool.enqueue_job("process_capture", capture.id)
        logger.info(f"Queued capture {capture.id} for processing")
    except Exception as e:
        # Log but don't fail the upload - processing can be retried via backlog
        logger.warning(f"Failed to queue capture {capture.id} for processing: {e}")

    return CaptureResponse(
        id=capture.id,
        filepath=capture.filepath,
        timestamp=capture.timestamp,
        monitor_index=capture.monitor_index,
        width=capture.width,
        height=capture.height,
        file_size=capture.file_size,
    )
```

Key design choices:
- Request parameter added to access app.state.arq_pool
- Job enqueue happens AFTER successful storage (data safety)
- Enqueue failure is logged but doesn't fail the upload
- Backlog cron job will catch any missed captures
- Response returns immediately (non-blocking)
  </action>
  <verify>
Run `python -c "from jarvis_server.api.captures import router; print('Captures router OK')"` in server/.venv.
Check captures.py contains "enqueue_job" and "process_capture".
  </verify>
  <done>Upload endpoint queues ARQ job for processing, non-blocking, graceful failure handling.</done>
</task>

</tasks>

<verification>
1. Main has ARQ pool: `grep -q "arq_pool" server/src/jarvis_server/main.py`
2. Captures enqueues: `grep -q "enqueue_job" server/src/jarvis_server/api/captures.py`
3. Import check: `python -c "from jarvis_server.api.captures import router; from jarvis_server.main import create_app"`
</verification>

<success_criteria>
- ARQ Redis pool initialized in app lifespan
- Pool accessible via request.app.state.arq_pool
- Upload endpoint enqueues "process_capture" job after storage
- Enqueue failure is logged but doesn't fail the upload
- Upload response returns immediately (non-blocking)
</success_criteria>

<output>
After completion, create `.planning/phases/02-searchable-memory-rag-core/02-05-SUMMARY.md`
</output>
