---
phase: 02-searchable-memory-rag-core
plan: 07
type: execute
wave: 2
depends_on: ["02-02", "02-06"]
files_modified:
  - server/src/jarvis_server/imports/api.py
  - server/src/jarvis_server/db/models.py
  - server/alembic/versions/003_add_conversations_table.py
  - server/src/jarvis_server/api/__init__.py
  - server/src/jarvis_server/main.py
autonomous: true

must_haves:
  truths:
    - "User can upload ChatGPT/Claude/Grok exports via API"
    - "Imported conversations are stored in database"
    - "Conversations are embedded and indexed in Qdrant"
  artifacts:
    - path: "server/src/jarvis_server/imports/api.py"
      provides: "Import API endpoints"
      exports: ["router"]
    - path: "server/src/jarvis_server/db/models.py"
      provides: "Conversation and ConversationMessage models"
      contains: "class Conversation"
    - path: "server/alembic/versions/003_add_conversations_table.py"
      provides: "Migration for conversations tables"
      contains: "conversations"
  key_links:
    - from: "server/src/jarvis_server/imports/api.py"
      to: "server/src/jarvis_server/imports/chatgpt.py"
      via: "parser usage"
      pattern: "parse_chatgpt_export"
    - from: "server/src/jarvis_server/imports/api.py"
      to: "server/src/jarvis_server/vector/qdrant.py"
      via: "embedding storage"
      pattern: "qdrant.*upsert"
---

<objective>
Create import API and database storage for AI chat conversations.

Purpose: Enable users to upload their ChatGPT/Claude/Grok exports, store in database, and index in Qdrant for search.

Output: Database models for conversations, API endpoints for file upload, processing pipeline that parses, stores, and embeds conversations.
</objective>

<execution_context>
@/home/sven/.claude/get-shit-done/workflows/execute-plan.md
@/home/sven/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-searchable-memory-rag-core/02-RESEARCH.md

# Parsers from 02-06
@.planning/phases/02-searchable-memory-rag-core/02-06-PLAN.md

# Embeddings from 02-02
@.planning/phases/02-searchable-memory-rag-core/02-02-PLAN.md

# Existing models
@server/src/jarvis_server/db/models.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add Conversation models and migration</name>
  <files>
server/src/jarvis_server/db/models.py
server/alembic/versions/003_add_conversations_table.py
  </files>
  <action>
Add database models for storing imported conversations.

**Add to server/src/jarvis_server/db/models.py:**
```python
class ConversationRecord(Base):
    """Imported AI conversation metadata."""

    __tablename__ = "conversations"

    # Primary key: UUID as string
    id: Mapped[str] = mapped_column(
        String(36), primary_key=True, default=lambda: str(uuid4())
    )

    # External ID from source system
    external_id: Mapped[str] = mapped_column(String(100), nullable=False)

    # Source: chatgpt, claude, grok
    source: Mapped[str] = mapped_column(String(20), nullable=False)

    # Conversation title
    title: Mapped[str] = mapped_column(String(500), nullable=False)

    # Full conversation text (for display)
    full_text: Mapped[str] = mapped_column(Text, nullable=False)

    # Message count
    message_count: Mapped[int] = mapped_column(Integer, nullable=False)

    # When conversation was created (from source)
    conversation_date: Mapped[datetime | None] = mapped_column(
        DateTime(timezone=True), nullable=True
    )

    # When imported into Jarvis
    imported_at: Mapped[datetime] = mapped_column(
        DateTime(timezone=True), server_default=func.now()
    )

    # Processing status for embedding
    processing_status: Mapped[str] = mapped_column(
        String(20), default="pending", nullable=False
    )

    __table_args__ = (
        Index("ix_conversations_source", "source"),
        Index("ix_conversations_date", "conversation_date"),
        Index("ix_conversations_external_id", "external_id", "source", unique=True),
    )
```

**server/alembic/versions/003_add_conversations_table.py:**
```python
"""Add conversations table for imported AI chats.

Revision ID: 003
Revises: 002
Create Date: 2026-01-24
"""
from alembic import op
import sqlalchemy as sa

revision = "003"
down_revision = "002"
branch_labels = None
depends_on = None


def upgrade() -> None:
    op.create_table(
        "conversations",
        sa.Column("id", sa.String(36), primary_key=True),
        sa.Column("external_id", sa.String(100), nullable=False),
        sa.Column("source", sa.String(20), nullable=False),
        sa.Column("title", sa.String(500), nullable=False),
        sa.Column("full_text", sa.Text, nullable=False),
        sa.Column("message_count", sa.Integer, nullable=False),
        sa.Column("conversation_date", sa.DateTime(timezone=True), nullable=True),
        sa.Column("imported_at", sa.DateTime(timezone=True), server_default=sa.func.now()),
        sa.Column("processing_status", sa.String(20), nullable=False, server_default="pending"),
    )

    op.create_index("ix_conversations_source", "conversations", ["source"])
    op.create_index("ix_conversations_date", "conversations", ["conversation_date"])
    op.create_index(
        "ix_conversations_external_id",
        "conversations",
        ["external_id", "source"],
        unique=True,
    )


def downgrade() -> None:
    op.drop_table("conversations")
```
  </action>
  <verify>
Run `python -c "from jarvis_server.db.models import ConversationRecord; print('ConversationRecord OK')"` in server/.venv.
Check migration exists: `ls server/alembic/versions/003*`
  </verify>
  <done>ConversationRecord model added with external_id/source uniqueness, migration creates conversations table with indices.</done>
</task>

<task type="auto">
  <name>Task 2: Create import API endpoint</name>
  <files>
server/src/jarvis_server/imports/api.py
server/src/jarvis_server/api/__init__.py
server/src/jarvis_server/main.py
  </files>
  <action>
Create import API for file uploads.

**server/src/jarvis_server/imports/api.py:**
```python
"""Import API endpoints for AI chat exports."""
import logging
import tempfile
from pathlib import Path
from typing import Literal
from uuid import uuid4

from fastapi import APIRouter, Depends, File, Form, HTTPException, UploadFile
from pydantic import BaseModel
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select
from sqlalchemy.dialects.postgresql import insert as pg_insert

from ..db.session import get_db
from ..db.models import ConversationRecord
from ..vector.qdrant import get_qdrant
from ..processing.embeddings import get_embedding_processor
from .chatgpt import parse_chatgpt_export
from .claude import parse_claude_export
from .grok import parse_grok_export

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/import", tags=["import"])

PARSERS = {
    "chatgpt": parse_chatgpt_export,
    "claude": parse_claude_export,
    "grok": parse_grok_export,
}


class ImportResponse(BaseModel):
    """Response from import endpoint."""
    imported: int
    skipped: int
    errors: int
    source: str


@router.post("/", response_model=ImportResponse)
async def import_conversations(
    file: UploadFile = File(...),
    source: Literal["chatgpt", "claude", "grok"] = Form(...),
    db: AsyncSession = Depends(get_db),
) -> ImportResponse:
    """Import AI chat export file.

    Supported formats:
    - chatgpt: conversations.json from ChatGPT data export
    - claude: ZIP or JSON from Claude export
    - grok: JSON from Grok export

    Conversations are stored in database and queued for embedding.
    Duplicate conversations (same external_id + source) are skipped.
    """
    if source not in PARSERS:
        raise HTTPException(status_code=400, detail=f"Unknown source: {source}")

    parser = PARSERS[source]

    # Save uploaded file to temp location
    suffix = ".zip" if source == "claude" and file.filename.endswith(".zip") else ".json"
    with tempfile.NamedTemporaryFile(suffix=suffix, delete=False) as tmp:
        content = await file.read()
        tmp.write(content)
        tmp_path = Path(tmp.name)

    imported = 0
    skipped = 0
    errors = 0

    try:
        qdrant = get_qdrant()
        embedder = get_embedding_processor()

        for conversation in parser(tmp_path):
            try:
                # Check if already exists
                existing = await db.execute(
                    select(ConversationRecord.id)
                    .where(ConversationRecord.external_id == conversation.id)
                    .where(ConversationRecord.source == source)
                )
                if existing.scalar_one_or_none():
                    skipped += 1
                    continue

                # Create database record
                record = ConversationRecord(
                    id=str(uuid4()),
                    external_id=conversation.id,
                    source=source,
                    title=conversation.title,
                    full_text=conversation.full_text,
                    message_count=conversation.message_count,
                    conversation_date=conversation.created_at,
                    processing_status="processing",
                )
                db.add(record)
                await db.flush()

                # Generate embedding
                if conversation.full_text.strip():
                    embedding = embedder.embed(conversation.full_text)

                    # Store in Qdrant
                    qdrant.upsert_capture(
                        capture_id=record.id,
                        dense_vector=embedding.dense.tolist(),
                        sparse_indices=embedding.sparse_indices,
                        sparse_values=embedding.sparse_values,
                        payload={
                            "timestamp": conversation.created_at.isoformat() if conversation.created_at else None,
                            "text_preview": conversation.full_text[:500],
                            "source": source,
                            "title": conversation.title,
                        },
                    )

                record.processing_status = "completed"
                imported += 1

            except Exception as e:
                logger.warning(f"Failed to import conversation {conversation.id}: {e}")
                errors += 1
                await db.rollback()

        await db.commit()

    finally:
        # Cleanup temp file
        tmp_path.unlink(missing_ok=True)

    logger.info(f"Import complete: {imported} imported, {skipped} skipped, {errors} errors")

    return ImportResponse(
        imported=imported,
        skipped=skipped,
        errors=errors,
        source=source,
    )


@router.get("/sources")
async def list_import_sources() -> dict:
    """List available import sources."""
    return {
        "sources": [
            {
                "id": "chatgpt",
                "name": "ChatGPT",
                "format": "conversations.json",
                "instructions": "ChatGPT Settings -> Data Controls -> Export data",
            },
            {
                "id": "claude",
                "name": "Claude",
                "format": "ZIP or JSON",
                "instructions": "Claude.ai -> Settings -> Export conversations",
            },
            {
                "id": "grok",
                "name": "Grok",
                "format": "JSON",
                "instructions": "accounts.x.ai -> Export data",
            },
        ]
    }
```

**Update server/src/jarvis_server/api/__init__.py:**
```python
from .captures import router as captures_router
from .health import router as health_router
from .search import router as search_router
from ..imports.api import router as import_router

__all__ = ["captures_router", "health_router", "search_router", "import_router"]
```

**Update server/src/jarvis_server/main.py:**
Add import router:
```python
from .api import captures_router, health_router, search_router, import_router

# In create_app() or wherever routers are registered:
app.include_router(import_router)
```
  </action>
  <verify>
Run `python -c "from jarvis_server.imports.api import router; print('Import router OK')"` in server/.venv.
Check main.py includes import_router.
  </verify>
  <done>Import API at POST /api/import/ accepts file uploads, parses conversations, stores in DB, embeds in Qdrant. GET /api/import/sources lists available sources.</done>
</task>

</tasks>

<verification>
1. Model: `python -c "from jarvis_server.db.models import ConversationRecord"`
2. Migration: `ls server/alembic/versions/003*`
3. API: `python -c "from jarvis_server.imports.api import router"`
4. Main includes import: `grep -q "import_router" server/src/jarvis_server/main.py`
</verification>

<success_criteria>
- ConversationRecord model with external_id/source uniqueness constraint
- Migration creates conversations table with proper indices
- POST /api/import/ accepts file + source, parses, stores, embeds
- Duplicate conversations (same external_id + source) are skipped
- GET /api/import/sources returns list of available sources
- Import router registered in FastAPI app
</success_criteria>

<output>
After completion, create `.planning/phases/02-searchable-memory-rag-core/02-07-SUMMARY.md`
</output>
