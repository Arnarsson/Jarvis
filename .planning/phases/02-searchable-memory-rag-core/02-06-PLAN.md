---
phase: 02-searchable-memory-rag-core
plan: 06
type: execute
wave: 1
depends_on: []
files_modified:
  - server/src/jarvis_server/imports/__init__.py
  - server/src/jarvis_server/imports/chatgpt.py
  - server/src/jarvis_server/imports/claude.py
  - server/src/jarvis_server/imports/grok.py
  - server/src/jarvis_server/imports/base.py
autonomous: true

must_haves:
  truths:
    - "ChatGPT conversations.json export can be parsed"
    - "Claude export ZIP can be parsed"
    - "Grok export JSON can be parsed"
    - "All parsers yield conversation dicts with consistent structure"
  artifacts:
    - path: "server/src/jarvis_server/imports/chatgpt.py"
      provides: "ChatGPT export parser"
      exports: ["parse_chatgpt_export"]
    - path: "server/src/jarvis_server/imports/claude.py"
      provides: "Claude export parser"
      exports: ["parse_claude_export"]
    - path: "server/src/jarvis_server/imports/grok.py"
      provides: "Grok export parser"
      exports: ["parse_grok_export"]
    - path: "server/src/jarvis_server/imports/base.py"
      provides: "Common types"
      exports: ["Conversation", "Message"]
  key_links:
    - from: "server/src/jarvis_server/imports/chatgpt.py"
      to: "orjson"
      via: "JSON parsing"
      pattern: "orjson.loads"
---

<objective>
Create parsers for AI chat export formats: ChatGPT, Claude, and Grok.

Purpose: Enable import of user's AI conversation history into Jarvis memory. These become searchable alongside screen captures.

Output: Parser modules for each format yielding standardized Conversation objects with messages, timestamps, and source metadata.
</objective>

<execution_context>
@/home/sven/.claude/get-shit-done/workflows/execute-plan.md
@/home/sven/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-searchable-memory-rag-core/02-RESEARCH.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create base types for imports</name>
  <files>
server/src/jarvis_server/imports/__init__.py
server/src/jarvis_server/imports/base.py
  </files>
  <action>
Create imports/ module with common types.

**server/src/jarvis_server/imports/__init__.py:**
```python
from .base import Conversation, Message
from .chatgpt import parse_chatgpt_export
from .claude import parse_claude_export
from .grok import parse_grok_export

__all__ = [
    "Conversation",
    "Message",
    "parse_chatgpt_export",
    "parse_claude_export",
    "parse_grok_export",
]
```

**server/src/jarvis_server/imports/base.py:**
```python
"""Base types for chat import parsers."""
from dataclasses import dataclass, field
from datetime import datetime
from typing import Optional


@dataclass
class Message:
    """A single message in a conversation."""

    role: str  # "user", "assistant", "system"
    content: str
    timestamp: Optional[datetime] = None

    def to_dict(self) -> dict:
        return {
            "role": self.role,
            "content": self.content,
            "timestamp": self.timestamp.isoformat() if self.timestamp else None,
        }


@dataclass
class Conversation:
    """A conversation with messages."""

    id: str
    title: str
    source: str  # "chatgpt", "claude", "grok"
    messages: list[Message] = field(default_factory=list)
    created_at: Optional[datetime] = None

    @property
    def full_text(self) -> str:
        """Concatenate all messages for embedding."""
        parts = [f"Title: {self.title}"]
        for msg in self.messages:
            parts.append(f"{msg.role.upper()}: {msg.content}")
        return "\n\n".join(parts)

    @property
    def message_count(self) -> int:
        return len(self.messages)

    def to_dict(self) -> dict:
        return {
            "id": self.id,
            "title": self.title,
            "source": self.source,
            "messages": [m.to_dict() for m in self.messages],
            "created_at": self.created_at.isoformat() if self.created_at else None,
        }
```
  </action>
  <verify>
Run `python -c "from jarvis_server.imports.base import Conversation, Message; print('Base types OK')"` in server/.venv.
  </verify>
  <done>Base types created: Message (role, content, timestamp), Conversation (id, title, source, messages) with full_text property for embedding.</done>
</task>

<task type="auto">
  <name>Task 2: Create ChatGPT export parser</name>
  <files>server/src/jarvis_server/imports/chatgpt.py</files>
  <action>
Create ChatGPT conversations.json parser.

**server/src/jarvis_server/imports/chatgpt.py:**
```python
"""ChatGPT export parser.

Parses the conversations.json file from ChatGPT data export.
Export obtained from: ChatGPT Settings -> Data Controls -> Export data
"""
import logging
from datetime import datetime
from pathlib import Path
from typing import Iterator
import orjson

from .base import Conversation, Message

logger = logging.getLogger(__name__)


def parse_chatgpt_export(filepath: str | Path) -> Iterator[Conversation]:
    """Parse ChatGPT conversations.json export file.

    Args:
        filepath: Path to conversations.json file

    Yields:
        Conversation objects for each conversation in the export
    """
    filepath = Path(filepath)

    with open(filepath, "rb") as f:
        data = orjson.loads(f.read())

    for conv in data:
        try:
            conversation = _parse_conversation(conv)
            if conversation and conversation.messages:
                yield conversation
        except Exception as e:
            logger.warning(f"Failed to parse conversation {conv.get('id', 'unknown')}: {e}")
            continue


def _parse_conversation(conv: dict) -> Conversation | None:
    """Parse a single conversation from ChatGPT export."""
    conv_id = conv.get("id")
    if not conv_id:
        return None

    title = conv.get("title", "Untitled")
    mapping = conv.get("mapping", {})

    messages = []
    for node_id, node in mapping.items():
        message_data = node.get("message")
        if not message_data:
            continue

        author = message_data.get("author", {})
        role = author.get("role")  # "user", "assistant", "system"
        if role not in ("user", "assistant", "system"):
            continue

        content = message_data.get("content", {})
        content_type = content.get("content_type")
        parts = content.get("parts", [])

        if content_type == "text" and parts:
            text = parts[0] if isinstance(parts[0], str) else ""
            if text.strip():
                timestamp = message_data.get("create_time")
                messages.append(
                    Message(
                        role=role,
                        content=text.strip(),
                        timestamp=datetime.fromtimestamp(timestamp) if timestamp else None,
                    )
                )

    # Sort by timestamp
    messages.sort(key=lambda m: m.timestamp or datetime.min)

    # Get conversation creation time from first message
    created_at = messages[0].timestamp if messages else None

    return Conversation(
        id=conv_id,
        title=title,
        source="chatgpt",
        messages=messages,
        created_at=created_at,
    )
```
  </action>
  <verify>
Run `python -c "from jarvis_server.imports.chatgpt import parse_chatgpt_export; print('ChatGPT parser OK')"` in server/.venv.
  </verify>
  <done>ChatGPT parser handles conversations.json format, extracts messages with roles and timestamps, yields Conversation objects.</done>
</task>

<task type="auto">
  <name>Task 3: Create Claude and Grok export parsers</name>
  <files>
server/src/jarvis_server/imports/claude.py
server/src/jarvis_server/imports/grok.py
  </files>
  <action>
Create parsers for Claude and Grok exports.

**server/src/jarvis_server/imports/claude.py:**
```python
"""Claude export parser.

Parses Claude conversation exports. Claude exports are ZIP files containing
JSON files with conversation data. Format may vary by export type.
"""
import logging
import zipfile
from datetime import datetime
from pathlib import Path
from typing import Iterator
import orjson

from .base import Conversation, Message

logger = logging.getLogger(__name__)


def parse_claude_export(filepath: str | Path) -> Iterator[Conversation]:
    """Parse Claude export ZIP file.

    Args:
        filepath: Path to Claude export ZIP file

    Yields:
        Conversation objects for each conversation in the export

    Note:
        Claude export format varies. This parser attempts to handle
        common structures but may need updates for specific formats.
    """
    filepath = Path(filepath)

    # Handle both ZIP and direct JSON
    if filepath.suffix == ".zip":
        yield from _parse_zip(filepath)
    elif filepath.suffix == ".json":
        yield from _parse_json_file(filepath)
    else:
        logger.warning(f"Unknown Claude export format: {filepath.suffix}")


def _parse_zip(filepath: Path) -> Iterator[Conversation]:
    """Parse Claude export ZIP file."""
    with zipfile.ZipFile(filepath, "r") as zf:
        for name in zf.namelist():
            if name.endswith(".json"):
                try:
                    with zf.open(name) as f:
                        data = orjson.loads(f.read())
                        yield from _parse_conversations(data, name)
                except Exception as e:
                    logger.warning(f"Failed to parse {name}: {e}")


def _parse_json_file(filepath: Path) -> Iterator[Conversation]:
    """Parse Claude JSON export file."""
    with open(filepath, "rb") as f:
        data = orjson.loads(f.read())
    yield from _parse_conversations(data, filepath.name)


def _parse_conversations(data: dict | list, source_name: str) -> Iterator[Conversation]:
    """Parse conversation data from Claude export."""
    # Handle list of conversations
    if isinstance(data, list):
        for i, conv in enumerate(data):
            conversation = _parse_single_conversation(conv, f"{source_name}_{i}")
            if conversation:
                yield conversation
        return

    # Handle single conversation object
    if isinstance(data, dict):
        # Check if this looks like a conversation
        if "chat_messages" in data or "messages" in data:
            conversation = _parse_single_conversation(data, source_name)
            if conversation:
                yield conversation
            return

        # Check for nested conversations
        for key in ["conversations", "chats"]:
            if key in data and isinstance(data[key], list):
                for i, conv in enumerate(data[key]):
                    conversation = _parse_single_conversation(conv, f"{source_name}_{i}")
                    if conversation:
                        yield conversation
                return


def _parse_single_conversation(conv: dict, fallback_id: str) -> Conversation | None:
    """Parse a single Claude conversation."""
    conv_id = conv.get("uuid") or conv.get("id") or fallback_id
    title = conv.get("name") or conv.get("title") or "Untitled"

    # Try different message field names
    raw_messages = conv.get("chat_messages") or conv.get("messages") or []

    messages = []
    for msg in raw_messages:
        role = msg.get("sender") or msg.get("role")
        if role == "human":
            role = "user"
        elif role == "assistant":
            role = "assistant"
        else:
            continue

        content = msg.get("text") or msg.get("content")
        if isinstance(content, list):
            # Handle structured content
            content = " ".join(
                part.get("text", "") for part in content if isinstance(part, dict)
            )

        if content and content.strip():
            timestamp_str = msg.get("created_at") or msg.get("timestamp")
            timestamp = None
            if timestamp_str:
                try:
                    timestamp = datetime.fromisoformat(timestamp_str.replace("Z", "+00:00"))
                except (ValueError, TypeError):
                    pass

            messages.append(
                Message(role=role, content=content.strip(), timestamp=timestamp)
            )

    if not messages:
        return None

    created_at = messages[0].timestamp if messages else None

    return Conversation(
        id=conv_id,
        title=title,
        source="claude",
        messages=messages,
        created_at=created_at,
    )
```

**server/src/jarvis_server/imports/grok.py:**
```python
"""Grok export parser.

Parses Grok conversation exports. Export obtained via accounts.x.ai
or third-party export tools.
"""
import logging
from datetime import datetime
from pathlib import Path
from typing import Iterator
import orjson

from .base import Conversation, Message

logger = logging.getLogger(__name__)


def parse_grok_export(filepath: str | Path) -> Iterator[Conversation]:
    """Parse Grok export JSON file.

    Args:
        filepath: Path to Grok export JSON file

    Yields:
        Conversation objects for each conversation in the export

    Note:
        Grok export format may vary. This parser handles common structures
        but may need updates for specific export tools.
    """
    filepath = Path(filepath)

    with open(filepath, "rb") as f:
        data = orjson.loads(f.read())

    # Handle list of conversations
    if isinstance(data, list):
        for i, conv in enumerate(data):
            conversation = _parse_conversation(conv, f"grok_{i}")
            if conversation:
                yield conversation
        return

    # Handle single conversation or wrapped structure
    if isinstance(data, dict):
        # Check for conversations list
        conversations = data.get("conversations") or data.get("chats") or [data]
        for i, conv in enumerate(conversations):
            conversation = _parse_conversation(conv, f"grok_{i}")
            if conversation:
                yield conversation


def _parse_conversation(conv: dict, fallback_id: str) -> Conversation | None:
    """Parse a single Grok conversation."""
    conv_id = conv.get("id") or conv.get("conversationId") or fallback_id
    title = conv.get("title") or conv.get("name") or "Grok Conversation"

    # Try different message field names
    raw_messages = conv.get("messages") or conv.get("turns") or []

    messages = []
    for msg in raw_messages:
        role = msg.get("role") or msg.get("author") or msg.get("sender")
        if role in ("human", "user"):
            role = "user"
        elif role in ("assistant", "grok", "ai"):
            role = "assistant"
        else:
            continue

        content = msg.get("content") or msg.get("text") or msg.get("message")
        if isinstance(content, list):
            content = " ".join(str(part) for part in content)

        if content and content.strip():
            timestamp_str = msg.get("timestamp") or msg.get("created_at")
            timestamp = None
            if timestamp_str:
                try:
                    if isinstance(timestamp_str, (int, float)):
                        timestamp = datetime.fromtimestamp(timestamp_str)
                    else:
                        timestamp = datetime.fromisoformat(timestamp_str.replace("Z", "+00:00"))
                except (ValueError, TypeError, OSError):
                    pass

            messages.append(
                Message(role=role, content=content.strip(), timestamp=timestamp)
            )

    if not messages:
        return None

    created_at = messages[0].timestamp if messages else None

    return Conversation(
        id=conv_id,
        title=title,
        source="grok",
        messages=messages,
        created_at=created_at,
    )
```

Key design choices:
- Flexible field name handling (different export tools use different names)
- Graceful degradation for missing/malformed data
- Consistent Conversation output across all sources
- orjson for fast JSON parsing of potentially large exports
  </action>
  <verify>
Run `python -c "from jarvis_server.imports import parse_claude_export, parse_grok_export; print('Claude/Grok parsers OK')"` in server/.venv.
  </verify>
  <done>Claude parser handles ZIP/JSON formats with flexible field names, Grok parser handles JSON with role normalization, both yield Conversation objects.</done>
</task>

</tasks>

<verification>
1. Base types: `python -c "from jarvis_server.imports.base import Conversation, Message"`
2. ChatGPT: `python -c "from jarvis_server.imports.chatgpt import parse_chatgpt_export"`
3. Claude: `python -c "from jarvis_server.imports.claude import parse_claude_export"`
4. Grok: `python -c "from jarvis_server.imports.grok import parse_grok_export"`
5. All from module: `python -c "from jarvis_server.imports import *; print('All imports OK')"`
</verification>

<success_criteria>
- Conversation and Message dataclasses with to_dict() and full_text property
- ChatGPT parser extracts from mapping structure with parts array
- Claude parser handles ZIP and JSON, flexible field names
- Grok parser handles JSON with role normalization
- All parsers gracefully skip malformed data with logging
- Consistent output: Iterator[Conversation] with messages sorted by timestamp
</success_criteria>

<output>
After completion, create `.planning/phases/02-searchable-memory-rag-core/02-06-SUMMARY.md`
</output>
