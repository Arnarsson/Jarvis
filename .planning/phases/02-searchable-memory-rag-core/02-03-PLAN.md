---
phase: 02-searchable-memory-rag-core
plan: 03
type: execute
wave: 2
depends_on: ["02-01", "02-02"]
files_modified:
  - server/src/jarvis_server/processing/pipeline.py
  - server/src/jarvis_server/processing/tasks.py
  - server/src/jarvis_server/processing/worker.py
  - server/src/jarvis_server/db/models.py
  - server/alembic/versions/002_add_processing_status.py
autonomous: true

must_haves:
  truths:
    - "ARQ worker processes captures in background without blocking uploads"
    - "Capture processing status is tracked in database"
    - "OCR text and embeddings are stored after processing"
  artifacts:
    - path: "server/src/jarvis_server/processing/tasks.py"
      provides: "ARQ task definitions"
      exports: ["process_capture", "process_backlog"]
    - path: "server/src/jarvis_server/processing/worker.py"
      provides: "ARQ worker configuration"
      exports: ["WorkerSettings"]
    - path: "server/alembic/versions/002_add_processing_status.py"
      provides: "Migration for processing_status column"
      contains: "processing_status"
  key_links:
    - from: "server/src/jarvis_server/processing/tasks.py"
      to: "server/src/jarvis_server/processing/ocr.py"
      via: "OCRProcessor usage"
      pattern: "ocr.*extract_text"
    - from: "server/src/jarvis_server/processing/tasks.py"
      to: "server/src/jarvis_server/vector/qdrant.py"
      via: "Qdrant upsert"
      pattern: "qdrant.*upsert"
---

<objective>
Create ARQ background processing pipeline for OCR and embedding of captures.

Purpose: Process screenshots asynchronously without blocking the upload API. Extracts text via OCR, generates embeddings, stores in Qdrant for search.

Output: ARQ tasks for processing captures, worker configuration, database migration for tracking processing status, pipeline orchestration.
</objective>

<execution_context>
@/home/sven/.claude/get-shit-done/workflows/execute-plan.md
@/home/sven/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-searchable-memory-rag-core/02-RESEARCH.md

# Prior phase work
@.planning/phases/02-searchable-memory-rag-core/02-01-PLAN.md
@.planning/phases/02-searchable-memory-rag-core/02-02-PLAN.md

# Existing database model
@server/src/jarvis_server/db/models.py
@server/alembic/versions/001_initial_schema.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add processing status to Capture model</name>
  <files>
server/src/jarvis_server/db/models.py
server/alembic/versions/002_add_processing_status.py
  </files>
  <action>
Update Capture model to track processing status.

**server/src/jarvis_server/db/models.py:**
Add new field to Capture class:
```python
# Processing status: pending, processing, completed, failed
processing_status: Mapped[str] = mapped_column(
    String(20), default="pending", nullable=False
)
```

Add index for efficient backlog queries:
```python
Index("ix_captures_processing_status", "processing_status"),
```

**server/alembic/versions/002_add_processing_status.py:**
Create migration:
```python
"""Add processing_status to captures table.

Revision ID: 002
Revises: 001
Create Date: 2026-01-24
"""
from alembic import op
import sqlalchemy as sa

revision = "002"
down_revision = "001"
branch_labels = None
depends_on = None


def upgrade() -> None:
    op.add_column(
        "captures",
        sa.Column("processing_status", sa.String(20), nullable=False, server_default="pending"),
    )
    op.create_index(
        "ix_captures_processing_status",
        "captures",
        ["processing_status"],
    )


def downgrade() -> None:
    op.drop_index("ix_captures_processing_status", table_name="captures")
    op.drop_column("captures", "processing_status")
```
  </action>
  <verify>
Run `python -c "from jarvis_server.db.models import Capture; print(Capture.__table__.columns.keys())"` - should include processing_status.
Check migration file exists: `ls server/alembic/versions/002*`
  </verify>
  <done>Capture model has processing_status field (pending/processing/completed/failed) with index for backlog queries.</done>
</task>

<task type="auto">
  <name>Task 2: Create processing pipeline and ARQ tasks</name>
  <files>
server/src/jarvis_server/processing/pipeline.py
server/src/jarvis_server/processing/tasks.py
  </files>
  <action>
Create pipeline orchestration and ARQ task definitions.

**server/src/jarvis_server/processing/pipeline.py:**
```python
"""Processing pipeline orchestration."""
import logging
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select, update

from ..db.models import Capture
from .ocr import OCRProcessor
from .embeddings import EmbeddingProcessor
from ..vector.qdrant import QdrantWrapper

logger = logging.getLogger(__name__)


async def process_single_capture(
    db: AsyncSession,
    capture_id: str,
    ocr: OCRProcessor,
    embedder: EmbeddingProcessor,
    qdrant: QdrantWrapper,
) -> dict:
    """Process a single capture: OCR -> embed -> store in Qdrant.

    Returns dict with status and details.
    """
    # 1. Load capture from database
    result = await db.execute(select(Capture).where(Capture.id == capture_id))
    capture = result.scalar_one_or_none()

    if not capture:
        return {"status": "not_found", "id": capture_id}

    # 2. Mark as processing
    await db.execute(
        update(Capture)
        .where(Capture.id == capture_id)
        .values(processing_status="processing")
    )
    await db.commit()

    try:
        # 3. Run OCR
        text = ocr.extract_text(capture.filepath)

        # 4. Update database with OCR text
        await db.execute(
            update(Capture)
            .where(Capture.id == capture_id)
            .values(ocr_text=text)
        )
        await db.commit()

        # 5. Generate embeddings (skip if no text)
        if text.strip():
            embedding = embedder.embed(text)

            # 6. Store in Qdrant
            qdrant.upsert_capture(
                capture_id=capture_id,
                dense_vector=embedding.dense.tolist(),
                sparse_indices=embedding.sparse_indices,
                sparse_values=embedding.sparse_values,
                payload={
                    "timestamp": capture.timestamp.isoformat(),
                    "filepath": capture.filepath,
                    "text_preview": text[:500],
                    "source": "screen",
                },
            )

        # 7. Mark completed
        await db.execute(
            update(Capture)
            .where(Capture.id == capture_id)
            .values(processing_status="completed")
        )
        await db.commit()

        return {
            "status": "processed",
            "id": capture_id,
            "text_length": len(text),
            "has_embedding": bool(text.strip()),
        }

    except Exception as e:
        logger.error(f"Failed to process capture {capture_id}: {e}")
        await db.execute(
            update(Capture)
            .where(Capture.id == capture_id)
            .values(processing_status="failed")
        )
        await db.commit()
        return {"status": "failed", "id": capture_id, "error": str(e)}


async def get_pending_captures(db: AsyncSession, limit: int = 100) -> list[str]:
    """Get capture IDs pending processing."""
    result = await db.execute(
        select(Capture.id)
        .where(Capture.processing_status == "pending")
        .order_by(Capture.timestamp.asc())
        .limit(limit)
    )
    return [row[0] for row in result.fetchall()]
```

**server/src/jarvis_server/processing/tasks.py:**
```python
"""ARQ task definitions for background processing."""
import logging
from arq import cron
from arq.connections import RedisSettings

from ..config import get_settings
from ..db.session import async_session_factory
from .ocr import get_ocr_processor
from .embeddings import get_embedding_processor
from ..vector.qdrant import get_qdrant
from .pipeline import process_single_capture, get_pending_captures

logger = logging.getLogger(__name__)


async def process_capture(ctx: dict, capture_id: str) -> dict:
    """ARQ task: Process a single capture."""
    async with async_session_factory() as db:
        result = await process_single_capture(
            db=db,
            capture_id=capture_id,
            ocr=ctx["ocr"],
            embedder=ctx["embedder"],
            qdrant=ctx["qdrant"],
        )
    logger.info(f"Processed capture {capture_id}: {result['status']}")
    return result


async def process_backlog(ctx: dict) -> dict:
    """ARQ task: Queue all pending captures for processing."""
    async with async_session_factory() as db:
        pending = await get_pending_captures(db, limit=100)

    redis = ctx["redis"]
    for capture_id in pending:
        await redis.enqueue_job("process_capture", capture_id)

    logger.info(f"Queued {len(pending)} captures for processing")
    return {"queued": len(pending)}
```

Update **server/src/jarvis_server/processing/__init__.py:**
```python
from .ocr import OCRProcessor, get_ocr_processor
from .embeddings import EmbeddingProcessor, EmbeddingResult, get_embedding_processor
from .pipeline import process_single_capture, get_pending_captures
from .tasks import process_capture, process_backlog

__all__ = [
    "OCRProcessor",
    "get_ocr_processor",
    "EmbeddingProcessor",
    "EmbeddingResult",
    "get_embedding_processor",
    "process_single_capture",
    "get_pending_captures",
    "process_capture",
    "process_backlog",
]
```
  </action>
  <verify>
Run `python -c "from jarvis_server.processing.tasks import process_capture, process_backlog; print('Tasks import OK')"` in server/.venv.
  </verify>
  <done>Processing pipeline created with process_single_capture orchestration, ARQ tasks for single capture and backlog processing.</done>
</task>

<task type="auto">
  <name>Task 3: Create ARQ worker configuration</name>
  <files>server/src/jarvis_server/processing/worker.py</files>
  <action>
Create ARQ worker settings module.

**server/src/jarvis_server/processing/worker.py:**
```python
"""ARQ worker configuration and settings."""
import logging
from arq import cron
from arq.connections import RedisSettings

from ..config import get_settings
from .ocr import get_ocr_processor
from .embeddings import get_embedding_processor
from ..vector.qdrant import get_qdrant
from .tasks import process_capture, process_backlog

logger = logging.getLogger(__name__)


class WorkerSettings:
    """ARQ worker settings for background processing."""

    # Register task functions
    functions = [process_capture, process_backlog]

    # Cron jobs for backlog processing
    cron_jobs = [
        cron(process_backlog, hour={0, 6, 12, 18}, minute=0),  # Every 6 hours
    ]

    # Worker limits
    max_jobs = 5  # Limit concurrent OCR jobs (memory intensive)
    job_timeout = 300  # 5 minutes per capture

    @staticmethod
    def redis_settings() -> RedisSettings:
        """Get Redis connection settings from config."""
        settings = get_settings()
        return RedisSettings(
            host=settings.redis_host,
            port=settings.redis_port,
        )

    @staticmethod
    async def on_startup(ctx: dict) -> None:
        """Initialize shared resources for worker."""
        logger.info("ARQ worker starting up...")

        # Initialize processing components (lazy-loaded on first use)
        ctx["ocr"] = get_ocr_processor()
        ctx["embedder"] = get_embedding_processor()
        ctx["qdrant"] = get_qdrant()

        # Ensure Qdrant collection exists
        from ..vector.qdrant import setup_captures_collection
        setup_captures_collection(ctx["qdrant"])

        logger.info("ARQ worker ready")

    @staticmethod
    async def on_shutdown(ctx: dict) -> None:
        """Cleanup on worker shutdown."""
        logger.info("ARQ worker shutting down...")


# Entry point for arq CLI: arq jarvis_server.processing.worker.WorkerSettings
```

Add entry point to **server/pyproject.toml** under [project.scripts]:
```toml
jarvis-worker = "arq:main"
```

This allows running: `arq jarvis_server.processing.worker.WorkerSettings`
  </action>
  <verify>
Run `python -c "from jarvis_server.processing.worker import WorkerSettings; print('Worker import OK')"` in server/.venv.
  </verify>
  <done>ARQ WorkerSettings configured with max_jobs=5, 5-min timeout, cron for backlog every 6 hours, startup initializes OCR/embedder/qdrant.</done>
</task>

</tasks>

<verification>
1. Migration exists: `ls server/alembic/versions/002*`
2. Model has field: `python -c "from jarvis_server.db.models import Capture; print('processing_status' in [c.name for c in Capture.__table__.columns])"`
3. Tasks import: `python -c "from jarvis_server.processing.tasks import process_capture, process_backlog"`
4. Worker import: `python -c "from jarvis_server.processing.worker import WorkerSettings"`
</verification>

<success_criteria>
- Capture model has processing_status field with migration
- Pipeline orchestrates: load -> OCR -> embed -> Qdrant upsert
- ARQ tasks defined for single capture and backlog processing
- Worker configuration with max_jobs=5, job_timeout=300s
- Cron job queues backlog every 6 hours
- Worker startup initializes shared OCR/embedder/qdrant instances
</success_criteria>

<output>
After completion, create `.planning/phases/02-searchable-memory-rag-core/02-03-SUMMARY.md`
</output>
